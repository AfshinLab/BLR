from collections import defaultdict
import os
import pickle
import sys

import pandas as pd
from snakemake.utils import validate

from blr.utils import ReadGroup, generate_chunks, symlink_relpath, parse_phaseblocks
from blr.cli.find_clusterdups import UnionFind

configfile: "blr.yaml"
validate(config, "config.schema.yaml")

# TODO include handling of reads from `longranger basic` that already have barcodes extracted and trimmed? See issue
#  in ema when discussed https://github.com/arshajii/ema/issues/15

# Hidden config options
config["ema_bins_dir"] = "fastq_bins"

# Import rules for trimming fastq files.
if config["library_type"] == "blr":
    include: "rules/trim_blr.smk"
elif config["library_type"] == "10x":
    include: "rules/trim_10x.smk"
elif config["library_type"] == "stlfr":
    include: "rules/trim_stlfr.smk"
elif config["library_type"] == "tellseq":
    include: "rules/trim_tellseq.smk"

# Import rules for phasing
include: "rules/phasing.smk"

# Create read group string to tag reads.
platform = "DNBSEQ" if config["library_type"] == "stlfr" else "ILLUMINA"
readgroup = ReadGroup(identifier=config["sample_nr"], library=config['library_type'], sample="20", platfrom_unit="unit1",
                      platform=platform)


# For parallelization, we split the initial mapped BAM file into non-overlapping "chunks",
# which are computed from the FASTA index file. 
chunks = generate_chunks(reference=config["genome_reference"], 
                         size=config["chunk_size"], 
                         phasing_contigs_string=config["phasing_contigs"],
                         contigs_skipped=config["contigs_skipped"])


def final_input(wildcards):
    files = [
        "multiqc_report.html",
        "final.bam",
        "final.bam.bai",
        "final.phased.bam",
        "final.phased.bam.bai",
        "final.phased.vcf.gz",
        "final.phased.vcf.gz.tbi",
        "final.naibr_sv_calls.bedpe",
        "final.sv_sizes.tsv"
    ]
    if not config["reference_variants"]:
        files.extend([
            "called.vcf.gz",
            "called.vcf.gz.tbi"
        ])
        if config["filter_variants"]:
            files.extend([
                "called.filtered.vcf.gz",
                "called.filtered.vcf.gz.tbi"
            ])
    return files


rule final:
   input: final_input


rule fastqc_raw_reads:
    """Creates fastqc reports from raw read files. Output names are automatically given by fastqc."""
    output:
        qc = "trimmed.barcoded.{nr,[12]}_fastqc.html",
        zip = "trimmed.barcoded.{nr,[12]}_fastqc.zip",
    input:
        reads = "trimmed.barcoded.{nr}.fastq.gz",
    log: "trimmed.barcoded.{nr,[12]}_fastqc.html.log"
    threads: 2  # Fix java.lang.OutOfMemoryError (https://github.com/s-andrews/FastQC/issues/24)
    shell:
        "fastqc"
        " {input.reads}"
        " -t {threads}"
        " 2> {log}"


def get_multiqc_input(wildcards):
    inputs = [
        "figures",
        "final.mosdepth.global.dist.txt",
        "final.mosdepth.summary.txt",
        "final.phaseblock_data.tsv",
        "final.whatshap_stats.tsv",
        "final.insert_size_metrics.txt",
        "final.insert_size_histogram.pdf",
        "final.phasing_stats.txt",
        "final.duplicate_metrics.txt",
        "final.molecule_lengths.tsv",
        "final.alignment_summary_metrics.txt",
        "final.sv_sizes.tsv",
        "blr_mqc.yaml",
        "ideogram_mqc.html",
    ]
    if config["read_mapper"] != "lariat":
        inputs.extend(["trimmed.barcoded.1_fastqc.html", "trimmed.barcoded.2_fastqc.html"])
    return inputs


rule multiqc_summarize:
    """Summarizes all reports into one html report. Automatically identifies input/gives output names but by adding
    more input files it controls when snakemake runs the multiqc summary.
    """
    output:
        multiqc_data_dir = directory("multiqc_data"),
        summarized_reports = "multiqc_report.html"
    input:
        get_multiqc_input
    log:
        "multiqc_report.html.log"
    shell:
        "multiqc . --template blr 2> {log}"


rule report_configs:
    """Translate configs to HTML to include in MultiQC report"""
    output:
        yaml = temp("blr_mqc.yaml")
    run:
        with open(output.yaml, "w") as configs:
            text = "id: 'run_configs'\n"
            text += "section_name: 'Run configs'\n"
            text += "plot_type: 'html'\n"
            text += "description: ' for current analysis. Describes which parameter values was used.'\n"
            text += "data: |\n"
            text += "\t<dl class=\"dl-horizontal\">\n".expandtabs(4)
            for param, value in config.items():
                text += f"\t\t<dt>{param}</dt><dd><samp>{value}</samp></dd>\n".expandtabs(4)
            text += "\t</dl>".expandtabs(4)
            print(text, file=configs)


rule generate_ideogram:
    """Generate ideogram"""
    output:
        html = temp("ideogram_mqc.html")
    input:
        phased_vcf = "final.phased.vcf.gz"
    params:
        assembly = config["ideogram_assembly"]
    script:
        "scripts/ideogram_html.py"


def plot_figures_input(wildcards):
    if config["library_type"] in {"blr", "tellseq"}:
        return "final.molecule_stats.filtered.tsv", "barcodes.clstr.gz"
    else:
        return "final.molecule_stats.filtered.tsv"


rule plot_figures:
    """Generate plots for final report."""
    output:
        images = directory("figures"),
        stats = "final.stats.txt"
    input: plot_figures_input
    log: "final.stats.txt.log"
    shell:
        "blr plot"
        " {input}"
        " -o {output.images} 2> {log} > {output.stats}"


rule map_sort_ema_bins:
    output:
        bam = temp("{dir}/ema-bin-{bin_nr,\d+}.bam")
    input:
        interleaved_fastq = "{dir}/ema-bin-{bin_nr}"
    threads: 4
    log:
        map = "{dir}/ema-bin-{bin_nr}.bam.mapping.log",
        sort = "{dir}/ema-bin-{bin_nr}.bam.sorting.log"
    params:
        tmpdir = " -T $TMPDIR" if "TMPDIR" in os.environ else ""
    shell:
        "ema align"
        " -t {threads}"
        " -d"
        " -p 10x"
        " -R {readgroup}"
        " -r {config[genome_reference]}"
        " -1 {input.interleaved_fastq}"
        " 2>> {log.map}"
        " |"
        " samtools sort "
        " -@ {threads}"
        " -m {config[heap_space]}G"
        " -O bam"
        " -l 0"
        "{params.tmpdir}"
        " -o {output.bam}"
        " - 2> {log.sort}"

rule map_sort_nobc:
    """Only for read_mapper=ema. Map and Sort non barcoded reads with BWA"""
    output:
        bam = "initialmapping_nobc.bam"
    input:
        r1_fastq = "trimmed.non_barcoded.1.fastq.gz",
        r2_fastq = "trimmed.non_barcoded.2.fastq.gz"
    threads: 4
    log:
        map = "initialmapping_nobc.bam.mapping.log",
        sort = "initialmapping_nobc.bam.sorting.log"
    params:
        tmpdir = " -T $TMPDIR" if "TMPDIR" in os.environ else ""
    shell: 
        "bwa mem"
        " -t {threads}"
        " -R {readgroup}"
        " {config[genome_reference]}"
        " {input.r1_fastq}"
        " {input.r2_fastq}"
        " 2> {log.map}"
        " |"
        " samtools sort "
        " -@ {threads}"
        " -m {config[heap_space]}G"
        " -O bam"
        " -l 0"
        "{params.tmpdir}"
        " -o {output.bam}"
        " - 2> {log.sort}"


if config["library_type"] in {"10x", "blr", "tellseq"} \
        and config["read_mapper"] == "ema" \
        and config["fastq_bins"] > 1:
    ruleorder: merge_mapped_ema_bins > map_sort
else:
    ruleorder: map_sort > merge_mapped_ema_bins


rule merge_mapped_ema_bins:
    output:
        bam = "initialmapping.bam",
        mapping_log = "initialmapping.bam.mapping.log",
        sorting_log = "initialmapping.bam.sorting.log",
    input:
        bams = expand(os.path.join(config['ema_bins_dir'], "ema-bin-{nr}.bam"),
                                   nr=[str(i).zfill(3) for i in range(config['fastq_bins'])]),
        bam_nobc = "initialmapping_nobc.bam"
    threads: 20
    shell:
        "samtools merge -cp -@ {threads} {output.bam} {input.bams} {input.bam_nobc}"
        " &&"
        " cat {config[ema_bins_dir]}/*.mapping.log > {output.mapping_log}" 
        " &&"
        " cat {config[ema_bins_dir]}/*.sorting.log > {output.sorting_log}"


rule map_sort:
    """Map reads using the aligner specified in configs. Output is sorted."""
    output:
        bam = "initialmapping.bam"
    input:
        r1_fastq = "trimmed.barcoded.1.fastq.gz",
        r2_fastq = "trimmed.barcoded.2.fastq.gz"
    threads: 20
    log:
        map = "initialmapping.bam.mapping.log",
        sort = "initialmapping.bam.sorting.log"
    run:
        commands = {
            "bwa":
                "bwa mem"
                " -t {threads}"
                " -R {readgroup}"
                " {config[genome_reference]}"
                " {input.r1_fastq}"
                " {input.r2_fastq}",
            "bowtie2":
                "bowtie2"
                " -p {threads}"
                " --rg-id {readgroup.identifier}"
                " --rg {readgroup.LB}"
                " --rg {readgroup.SM}"
                " --rg {readgroup.PU}"
                " --rg {readgroup.PL}"
                " --reorder"
                " --maxins 2000"
                " -x {config[genome_reference]}"
                " -1 {input.r1_fastq}"
                " -2 {input.r2_fastq}",
            "minimap2":
                "minimap2"
                " -ax sr"
                " -R {readgroup}"
                " -t {threads}"
                " {config[genome_reference]}"
                " {input.r1_fastq}"
                " {input.r2_fastq}",
            "ema":
                "ema align"
                " -1 <(pigz -cd {input.r1_fastq})"
                " -2 <(pigz -cd {input.r2_fastq})"
                " -R {readgroup}"
                " -r {config[genome_reference]}"
                " -t {threads}"
                " -i {config[sample_nr]}"
                " -p 10x",
            "lariat":
                # Experimental addition!
                # Lariat creates a output directory with three files:
                #  - 000000-chrA_0000000000_pos_bucketed.bam -> Mapped barcode tagged reads for chunk (multiple if run
                #    on several chunks)
                #  - ZZZ_unmapped_pos_bucketed.bam --> Unmapped reads
                #  - bc_sorted_bam.bam --> Concatenated BAM with all reads.
                # Only the `bc_sorted_bam.bam` is used currently and the rest deleted on step completion. Lariat does not
                # add read groups (or I cannot figure out how) so this is done separately.
                "mkdir -p lariat_tmp"
                " && "
                "trap 'rm -rf lariat_tmp' EXIT"
                " && "
                "lariat"
                " -reads={input.r1_fastq}"
                " -genome={config[genome_reference]}"
                " -threads={threads}"
                " -trim_length=0"
                " -first_chunk=True"
                " -output=lariat_tmp"
                " -sample_id={readgroup.sample} &> {log.map}"
                " && "  
                "samtools addreplacerg"  
                " -r {readgroup.ID}"
                " -r {readgroup.LB}"
                " -r {readgroup.SM}"
                " -r {readgroup.PL}"
                " -r {readgroup.PU}"
                " lariat_tmp/bc_sorted_bam.bam"
        }
        command = commands[config["read_mapper"]]

        # Use temporary directory for intermediate file if TMPDIR set in environment.
        tmpdir = " -T $TMPDIR" if "TMPDIR" in os.environ else ""

        shell(
            command + " 2>> {log.map} | "
            "samtools sort -"
            " -@ {threads}"
            " -m {config[heap_space]}G"
            " -o {output.bam}" +
            tmpdir +
            " 2> {log.sort}"
        )


rule make_chunk_beds:
    output:
        expand("chunks/{chunk[0].name}.bed", chunk=chunks["all"])
    run:
        for chunk in chunks["all"]:
            with open(f"chunks/{chunk[0].name}.bed", "w") as f:
                for chromosome in chunk:
                    print(chromosome.name, 0, chromosome.length, sep="\t", file=f)


skip_tagbam = (config["library_type"] == "10x" and config["read_mapper"] == "ema") or \
              config["read_mapper"] == "lariat"


rule split_into_chunks:
    output:
        bam = temp("chunks/{chunk}.sorted.tag.bam") if skip_tagbam else pipe("chunks/{chunk}.sorted.bam")
    input:
        bam = "initialmapping.bam",
        bai = "initialmapping.bam.bai",
        bed = "chunks/{chunk}.bed",
    shell:
        "samtools view -M -L {input.bed} -o {output.bam} {input.bam}"


rule get_unmapped_reads:
    output:
        bam = temp("unmapped.bam")
    input:
        bam = "initialmapping.bam",
        bai = "initialmapping.bam.bai"
    log: "unmapped.bam.log"
    params:
        tag = ">" if skip_tagbam else f"| blr tagbam -  -m {config['read_mapper']} -s {config['sample_nr']} -o",
        outtype = "-bh" if skip_tagbam else "-h"
    shell:
        "samtools view {params.outtype} {input.bam} '*' {params.tag} {output.bam} 2> {log}"


rule tagbam:
    output:
        bam = temp("{base}.tag.bam")
    input:
        bam = "{base}.bam"
    log:
        "{base}.tag.bam.log"
    shell:
        "blr tagbam "
        " -o {output.bam}"
        " --mapper {config[read_mapper]}"
        " --sample-nr {config[sample_nr]}"
        " {input.bam}  2> {log}"


rule find_clusterdups:
    """Find cluster duplicates defined as two separate barcodes sharing duplicate read pair"""
    output:
        pickle = temporary("{base}.clusterdups.pickle")
    input:
        bam = "{base}.bam"
    log: "{base}.clusterdups.pickle.log"
    shell:
        "blr find_clusterdups"
        " {input.bam}"
        " --output-pickle {output.pickle}"
        " --min-mapq {config[min_mapq]}"
        " --library-type {config[library_type]}"
        " --window {config[window_size]}"
        " --quantile-threshold 0.99"
        " -b {config[cluster_tag]} 2> {log}"


rule get_barcode_merges:
    """Merge graphs of connected barcodes from all chunks to get CSV of barcodes to merge."""
    output:
        merges = "final.barcode-merges.csv"
    input:
        pickle = expand("chunks/{chunk[0].name}.sorted.tag.clusterdups.pickle", chunk=chunks["primary"])
    run:
        uf = UnionFind()
        for filepath in input.pickle:
            with open(filepath, "rb") as file:
                uf_file = UnionFind.from_dict(pickle.load(file))
                uf.update(uf_file)

        with open(output.merges, 'w') as file:
            for old_barcode, new_barcode in uf.items():
                if old_barcode != new_barcode:
                    print(old_barcode, new_barcode, sep=",", file=file)


rule merge_clusterdups:
    """Merge cluster duplicates defined as two separate cluster sharing """
    output:
        bam = temp("{base}.bcmerge.bam"),
    input:
        bam = "{base}.bam",
        merges = "final.barcode-merges.csv"
    log: "{base}.bcmerge.bam.log"
    shell:
        "blr merge_clusterdups"
        " {input.bam}"
        " {input.merges}"
        " -o {output.bam}"
        " -b {config[cluster_tag]} 2> {log}"


rule mark_duplicates:
    """Mark duplicates within barcodes clusters."""
    output:
        bam = temp("{base}.mkdup.bam"),
        metrics = "{base}.mkdup_metrics.txt"
    input:
        bam = "{base}.bam"
    log: "{base}.mkdup.bam.log"
    shell:
        "picard -Xmx{config[heap_space]}g MarkDuplicates"
        " INPUT={input.bam}"
        " OUTPUT={output.bam}"
        " METRICS_FILE={output.metrics}"
        " READ_ONE_BARCODE_TAG={config[cluster_tag]}"
        " READ_TWO_BARCODE_TAG={config[cluster_tag]}"
        # Running without `USE_JDK_DEFLATER=true` & `USE_JDK_INFLATER=true` causes fatal error. See issue: 
        # https://github.com/broadinstitute/picard/issues/1329
        " USE_JDK_DEFLATER=true"
        " USE_JDK_INFLATER=true"
        " ASSUME_SORTED=true"
        " MAX_RECORDS_IN_RAM=250000"
        " VALIDATION_STRINGENCY=LENIENT"
        " &> {log}"


rule buildmolecules:
    """Groups reads into molecules depending on their genomic position and barcode"""
    output:
        bam = temp("{base}.mol.bam"),
        stats = temp("{base}.molecule_stats.tsv")
    input:
        bam = "{base}.bam"
    log: "{base}.mol.bam.log"
    shell:
        "blr buildmolecules"
        " {input.bam}"
        " -o {output.bam}"
        " --stats-tsv {output.stats}"
        " -m {config[molecule_tag]}"
        " -b {config[cluster_tag]}"
        " --window {config[window_size]}"
        " --min-mapq {config[min_mapq]}"
        " --library-type {config[library_type]}"
        " 2> {log}"


rule concat_molecule_stats:
    output:
        tsv = "final.molecule_stats.tsv"
    input:
        tsv = expand("chunks/{chunk[0].name}.sorted.tag.bcmerge.mkdup.molecule_stats.tsv", chunk=chunks["primary"])
    run:
        dfs = list()
        for nr, file in enumerate(input.tsv):
            try:
                df = pd.read_csv(file, sep="\t")
            except pd.errors.EmptyDataError:
                continue

            df["ChunkID"] = nr
            dfs.append(df)

        concat = pd.concat(dfs, ignore_index=True)
        concat.to_csv(output.tsv, sep="\t", index=False)

rule get_barcodes_to_filter:
    output:
         tsv = "final.barcodes_filtered_out.tsv"
    input:
         tsv = "final.molecule_stats.tsv"
    run:
        molecules = pd.read_csv(input.tsv, sep="\t")
        molecules_per_barcode = molecules.groupby("Barcode", sort=False)["Barcode"].count()
        barcodes_to_filter = molecules_per_barcode[molecules_per_barcode > config["max_molecules_per_bc"]].index.to_list()
        with open(output.tsv, "w") as file:
            print("\n".join(barcodes_to_filter), file=file)


rule filter_molecule_stats:
    """Remove barcodes that are to be filtered out from stats"""
    output:
        tsv = "final.molecule_stats.filtered.tsv"
    input:
        tsv = "final.molecule_stats.tsv",
        barcodes = "final.barcodes_filtered_out.tsv"
    run:
        molecules = pd.read_csv(input.tsv, sep="\t")
        barcodes_to_filter = pd.read_csv(input.barcodes, sep="\t", names=["Barcode"])
        molecules_filtered = molecules[~molecules["Barcode"].isin(barcodes_to_filter["Barcode"])]
        molecules_filtered.to_csv(output.tsv, sep="\t", index=False)


# filterclusters creates a .bai file directly
ruleorder: filterclusters > index_bam


rule filterclusters:
    """Filter clusters based on number of molecules. Remove duplicates from BAM"""
    output:
        bam = "{base}.filt.bam",
        bai = "{base}.filt.bam.bai"
    input:
        bam = "{base}.bam",
        barcodes = "final.barcodes_filtered_out.tsv"
    log: "{base}.filt.bam.log"
    shell:
        "blr filterclusters"
        " {input.bam}"
        " {input.barcodes}"
        " -m {config[molecule_tag]}"
        " -b {config[cluster_tag]}"
        " 2> {log} |"
        " tee {output.bam}"
        " |"
        " samtools index  - {output.bai}"


rule bam_to_fastq:
    """Convert final BAM file to FASTQ files for read 1 and 2"""
    output:
        r1_fastq = "reads.1.final.fastq.gz",
        r2_fastq = "reads.2.final.fastq.gz"
    input:
        bam = "final.bam"
    log: "reads.1.final.fastq.gz.log"
    threads: 20
    shell:
        "samtools fastq"
        " -@ {threads}"
        " -T {config[cluster_tag]},{config[sequence_tag]}"
        " {input.bam}"
        " -1 {output.r1_fastq}"
        " -2 {output.r2_fastq} 2> {log}"


rule recal_base_qual_scores:
    """Recalibrate base calling qualities"""
    output:
        recal_table = "{base}.bsqr_table.txt"
    input:
        bam = "{base}.bam"
    log: "{base}.bsqr_table.txt.log"
    shell:
        "gatk --java-options -Xmx{config[heap_space]}g BaseRecalibrator"
        " -R {config[genome_reference]}"
        " --known-sites {config[dbSNP]}"
        " -I {input.bam}"
        " -O {output.recal_table}"
        " 2> {log}"


rule apply_recal:
    """Adjusts BAM with results from gatk base score recalibration"""
    output:
        recal_bam = "{base}.BQSR.bam"
    input:
        bam = "{base}.bam",
        recal_table = "{base}.bsqr_table.txt"
    log: "{base}.BQSR.bam.log"
    shell:
        "gatk --java-options -Xmx{config[heap_space]}g ApplyBQSR"
        " --bqsr-recal-file {input.recal_table}"
        " -R {config[genome_reference]}"
        " -I {input.bam}"
        " -O {output.recal_bam}"
        " 2> {log}"


def get_input_calling_bam(wildcards):
    basename = f"{wildcards.base}.sorted.tag.bcmerge.mkdup.mol.filt"
    if config["variant_caller"] == "gatk" and config["BQSR"]:
        basename += ".BQSR"
    return f"{basename}.bam"


rule symlink_calling_bam:
    # The BAM file (with all preprocesing applied) used for calling variants
    output:
        bam = "{base}.calling.bam"
    input:
        bam = get_input_calling_bam
    run:
        symlink_relpath(input.bam, output.bam)


rule index_bam:
    output:
        bai = "{base}.bam.bai"
    input:
        bam = "{base}.bam"
    shell:
        "samtools index"
        " {input.bam}"
        " {output.bai}"


rule compress_vcf:
    output:
        vcf_gz = "{base}.vcf.gz",
    input:
        vcf = "{base}.vcf"
    shell:
        "bgzip -c {input.vcf} > {output.vcf_gz}"


rule index_vcf:
    output:
        "{base}.vcf.gz.tbi"
    input:
        vcf = "{base}.vcf.gz"
    shell:
        "tabix -p vcf {input.vcf}"


rule call_variants:
    """Call variants using the selected variant caller from configs."""
    output:
        vcf = temporary("{base}.variants.called.vcf")
    input:
        bam = "{base}.calling.bam",
        bai = "{base}.calling.bam.bai",
    log: "{base}.variants.called.vcf.log"
    threads: 2 if config["variant_caller"] in {"gatk", "bcftools"} else 1
    run:
        heap_space = config["heap_space"] * threads
        commands = {
            "freebayes":
                 "freebayes"
                 " -f {config[genome_reference]}"
                 " {input.bam}"
                 " 2> {log}"
                 " | "
                 # Filter out variants with allele count above 0 as these are detected as polyploid by hapcut2
                 "vcffilter"
                 " -f"
                 " 'AC > 0'"
                 " 1> {output.vcf} 2>> {log}",
            "bcftools":
                "bcftools mpileup"
                " -f {config[genome_reference]}"
                " {input.bam}"
                " --threads {threads}"
                " 2> {log}"
                " | "
                "bcftools call"
                " -m"
                " -v"
                " -O v"
                " --ploidy GRCh38"
                " --threads {threads}"
                " -o {output.vcf} 2>> {log}",
            "gatk":
                "gatk --java-options '-Xmx{heap_space}g -XX:ParallelGCThreads={threads}' HaplotypeCaller"
                " -R {config[genome_reference]}"
                " -I {input.bam}"
                " -O {output.vcf}"
                " --create-output-variant-index false 2> {log}",
            # This option is currently not included in the pipeline documentation, but it will run on any system which
            # - runs Linux (it's not maintained for any other system)
            # - has deepvariant installed (e.g. through conda)
            # TODO: Add a conditional installation of deepvariant for Linux systems and add to pipeline docs.
            "deepvariant":
                "deepvariant"
	            " --model_type=WGS"
	            " --output_vcf={output.vcf}"
	            " --reads={input.bam}"
	            " --ref={config[genome_reference]}"
                }
        shell(commands[config["variant_caller"]])


rule extract_called:
    """Extract SNPs and INDELs from called varinats"""
    output:
        vcf = temporary("{base}.variants.called.{type,(SNP|INDEL)}.vcf")
    input:
        vcf = "{base}.variants.called.vcf"
    log: "{base}.variants.called.{type,(SNP|INDEL)}.vcf.log"
    shell:
        "gatk SelectVariants"
        " -V {input.vcf}"
        " -select-type {wildcards.type}"
        " --create-output-variant-index false"
        " -O {output.vcf} 2> {log}"


rule filter_called_snps:
    """Filter SNPs. Thresholds partly based on Chen et al. 2019 (doi:10.1101/gr.245126.118)"""
    output:
        vcf = temporary("{base}.variants.called.SNP.filtered.vcf")
    input:
        vcf = "{base}.variants.called.SNP.vcf"
    log: "{base}.variants.called.SNP.filtered.vcf.log"
    shell:
        # TODO Include genotype filtration for AD < 2
        "gatk VariantFiltration"
        " -V {input.vcf}"
        " --filter-name 'lowQUAL'"
        " --filter-expression 'QUAL < 15'"
        " --filter-name 'highMQRankSum'"
        " --filter-expression 'MQRankSum > 6.0'"
        " --filter-name 'snpLowAF'"
        " --filter-expression 'AF < 0.15'"
        " --create-output-variant-index false"
        " -O {output.vcf} 2> {log}"


rule filter_called_indels:
    """Filter INDELs. Thresholds partly based on Chen et al. 2019 (doi:10.1101/gr.245126.118)"""
    output:
        vcf = temporary("{base}.variants.called.INDEL.filtered.vcf")
    input:
        vcf = "{base}.variants.called.INDEL.vcf"
    log: "{base}.variants.called.INDEL.filtered.vcf.log"
    shell:
        # TODO Include genotype filtration for AD < 4
        "gatk VariantFiltration"
        " -V {input.vcf}"
        " --filter-name 'lowQUAL'"
        " --filter-expression 'QUAL < 15'"
        " --filter-name 'highMQRankSum'"
        " --filter-expression 'MQRankSum > 6.0'"
        " --filter-name 'indelLowAF'"
        " --filter-expression 'AF < 0.25'"
        " --create-output-variant-index false"
        " -O {output.vcf} 2> {log}"


rule merge_filtered_snps_indels:
    output:
        vcf = temporary("{base}.variants.called.filtered.vcf")
    input:
        vcf_snps = "{base}.variants.called.SNP.filtered.vcf",
        vcf_indels = "{base}.variants.called.INDEL.filtered.vcf"
    log: "{base}.variants.called.filtered.vcf.log"
    shell:
        "picard MergeVcfs"
        " --CREATE_INDEX false"
        " -O {output.vcf}"
        " -I {input.vcf_snps}"
        " -I {input.vcf_indels} 2> {log}"


def get_phase_input_vcf(wildcards):
    if config["reference_variants"]:
        return {
            "vcf": config["reference_variants"],
            "bed": f"chunks/{wildcards.chunk}.bed",
        }
    else:
        if config["filter_variants"]:
            return {"vcf": f"chunks/{wildcards.chunk}.variants.called.filtered.vcf"}
        else:
            return {"vcf": f"chunks/{wildcards.chunk}.variants.called.vcf"}


rule split_or_symlink_phase_input_vcf:
    output:
        vcf = temp("chunks/{chunk}.phaseinput.vcf")
    input:
        unpack(get_phase_input_vcf)
    run:
        if config["reference_variants"]:
            # TODO
            # bcftools view -T is possibly a bit inefficient (use an indexed VCF instead)
            shell("bcftools view -o {output.vcf} -T {input.bed} {input.vcf}")
        else:
            symlink_relpath(input.vcf, output.vcf)


def bams_to_merge(wildcards):
    if wildcards.phased == "":
        input = [f"chunks/{chunk[0].name}.calling.{{phased}}bam" for chunk in chunks["all"]]
    else:
        input = [f"chunks/{chunk[0].name}.calling.{{phased}}bam" for chunk in chunks["phased"]]
        input.extend([f"chunks/{chunk[0].name}.calling.bam" for chunk in chunks["not_phased"]])
    input.append("unmapped.bam")
    return input


rule merge_bams:
    output:
        bam = "final.{phased,(phased.|)}bam"
    input:
        bams = ancient(bams_to_merge)
    shell:
        "samtools cat -o {output.bam} {input.bams}"


def vcfs_to_merge(wildcards):
    input = [f"chunks/{chunk[0].name}.calling.phased.vcf" for chunk in chunks["phased"]]
    filt = "filtered." if config["filter_variants"] else ""
    input.extend([f"chunks/{chunk[0].name}.variants.called.{filt}vcf" for chunk in chunks["primary_not_phased"]])
    return input


rule concat_phased_vcfs:
    output:
        vcf = temp("final.phased.vcf")
    input:
        vcfs_to_merge
    shell:
        "bcftools concat -o {output.vcf} {input}"


rule concat_called_vcfs:
    output:
        vcf = temp("called.{filtered,(filtered.|)}vcf")
    input:
        vcf = expand("chunks/{chunk[0].name}.variants.called.{{filtered}}vcf", chunk=chunks["primary"])
    shell:
        "bcftools concat -o {output.vcf} {input.vcf}"


rule mosdepth:
    """Calculate depth stats"""
    output:
        "final.mosdepth.global.dist.txt",
        "final.mosdepth.summary.txt",
    input:
        bam = "final.bam",
        bai = "final.bam.bai"
    params:
        prefix = "final"
    threads: workflow.cores * 0.75
    shell:
        "mosdepth"
        " --threads {threads}"
        " --no-per-base"
        " {params.prefix}"
        " {input.bam}"


rule aggregate_phaseblock_data:
    output:
        tsv = "final.phaseblock_data.tsv"
    input:
        phase = expand("chunks/{chunk[0].name}.calling.phase", chunk=chunks["phased"])
    run:
        data = list()
        with open(output.tsv, "w") as output_tsv:
            print("Variants spanned", "Variants phased", "Length", "Fragments", sep="\t", file=output_tsv)
            for input_file in input.phase:
                with open(input_file) as file:
                    for p in parse_phaseblocks(file):
                        print(p.snv_span, p.phased_snvs, p.length, p.fragments, sep="\t", file=output_tsv)


rule aggregate_molecule_lengths:
    output:
        tsv = "final.molecule_lengths.tsv"
    input:
        tsv = "final.molecule_stats.filtered.tsv"
    run:
        molecules = pd.read_csv(input.tsv, sep="\t")
        bins = range(0, max(molecules["Length"])+1000, 1000)
        molecules["Bin"] = pd.cut(molecules["Length"], bins=bins, labels=bins[:-1])
        binned_lengths = molecules.groupby("Bin", as_index=False)["Length"].sum()
        binned_lengths.columns = ["Bin", "LengthSum"]
        binned_lengths = binned_lengths[binned_lengths["LengthSum"] > 0]  # Remove zero entries
        binned_lengths.to_csv(output.tsv, sep="\t", index=False)


rule whatshap_stats:
    """Calculate phaseing stats related to variants and phaseblocks."""
    output:
        tsv = "final.whatshap_stats.tsv"
    input:
        phased_vcf = "final.phased.vcf.gz",
        phased_vcf_index = "final.phased.vcf.gz.tbi"
    log: "final.whatshap_stats.tsv.log"
    shell:
         "whatshap stats"
         " --tsv {output.tsv}"
         " {input.phased_vcf} > {log}"


rule collect_insert_size_metrics:
    """Collect insert size metrics for BAM file."""
    output:
        metrics = "final.insert_size_metrics.txt",
        hist = "final.insert_size_histogram.pdf",
    input:
        bam = "final.bam"
    log: "inal.insert_size_metrics.txt.log"
    shell:
        "picard -Xmx{config[heap_space]}g CollectInsertSizeMetrics"
        " I={input.bam}"
        " O={output.metrics}"
        " H={output.hist}"
        " VALIDATION_STRINGENCY=LENIENT 2> {log}"


rule collect_duplicate_metrics:
    output:
        metrics = "final.duplicate_metrics.txt"
    input:
        bam = "final.bam"
    log: "final.duplicate_metrics.txt.log"
    shell:
        "picard -Xmx{config[heap_space]}g CollectDuplicateMetrics"
        " I={input.bam}"
        " M={output.metrics}"
        " ASSUME_SORTED=true"
        " VALIDATION_STRINGENCY=LENIENT 2> {log}"


rule collect_alignment_summary_metrics:
    output:
        metrics = "final.alignment_summary_metrics.txt"
    input:
        bam = "final.bam"
    log: "final.alignment_summary_metrics.txt.log"
    shell:
        "picard -Xmx{config[heap_space]}g CollectAlignmentSummaryMetrics"
        " I={input.bam}"
        " O={output.metrics}"
        " MAX_INSERT_SIZE=2000"
        " ASSUME_SORTED=true"
        " VALIDATION_STRINGENCY=LENIENT 2> {log}"


rule concat_lsv_calls:
    output:
        tsv = "final.naibr_sv_calls.tsv"
    input:
        tsv = expand("chunks/{chunk[0].name}.naibr_sv_calls.tsv", chunk=chunks["primary"])
    run:
        dfs = list()
        for file in input.tsv:
            try:
                dfs.append(pd.read_csv(file, sep="\t"))
            except pd.errors.EmptyDataError:
                continue

        concat = pd.concat(dfs, ignore_index=True)
        concat.to_csv(output.tsv, sep="\t", index=False)


ruleorder: compress_vcf > gzip


rule gzip:
    """Compress file using pigz"""
    output: 
        "{file}.gz"
    input: 
        "{file}"
    shell:
        "pigz --stdout {input} > {output}"
