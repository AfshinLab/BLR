import sys
import pandas as pd
from snakemake.utils import validate
from blr.utils import ReadGroup, parse_fai, chromosome_chunks, symlink_relpath, parse_phaseblocks
import os

configfile: "blr.yaml"
validate(config, "config.schema.yaml")

# TODO include handling of reads from `longranger basic` that already have barcodes extracted and trimmed? See issue
#  in ema when discussed https://github.com/arshajii/ema/issues/15

# Import rules for trimming fastq files.
if config["library_type"] == "blr":
    include: "rules/trim_blr.smk"
elif config["library_type"] == "10x":
    include: "rules/trim_10x.smk"
elif config["library_type"] == "stlfr":
    include: "rules/trim_stlfr.smk"

# Import rules for phasing
include: "rules/phasing.smk"

# Create read group string to tag reads.
platform = "DNBSEQ" if config["library_type"] == "stlfr" else "ILLUMINA"
readgroup = ReadGroup(identifier="1", library=config['library_type'], sample="20", platfrom_unit="unit1",
                      platform=platform)


# For parallelization, we split the initial mapped BAM file into non-overlapping "chunks",
# which are computed from the FASTA index file
if config["genome_reference"] is not None:
    try:
        with open(config["genome_reference"] + ".fai") as f:
            chunks = list(chromosome_chunks(parse_fai(f), size=config["chunk_size"]))
    except FileNotFoundError as e:
        sys.exit(
            f"The genome index file {config['genome_reference']} is missing. "
            "Please create it with 'samtools faidx'"
        )
else:
    chunks = []


def get_final_targets(wildcards):
    """Define final file names based on configs."""
    input_files = [
        "reads.1.final.fastq.gz",
        "reads.2.final.fastq.gz",
        "multiqc_report.html",
        "final.bam",
    ]
    if config["phasing_ground_truth"]:
        input_files.append("final.phasing_stats.txt")

    return input_files


rule final:
   input:
        get_final_targets


rule fastqc_raw_reads:
    """Creates fastqc reports from raw read files. Output names are automatically given by fastqc."""
    output:
        qc = expand("trimmed.barcoded.{nr}_fastqc.html", nr=(1,2)),
        zip = expand("trimmed.barcoded.{nr}_fastqc.zip", nr=(1,2))
    input:
        reads = expand("trimmed.barcoded.{nr}.fastq.gz", nr=(1,2)),
    log:
        "fastqc_raw_reads.log"
    shell:
        "fastqc"
        " {input.reads}"
        " 2> {log}"


def get_multiqc_input(wildcards):
    inputs = [
        "trimmed.barcoded.1_fastqc.html",
        "trimmed.barcoded.2_fastqc.html",
        "final.bam",
        "figures",
        "final.mosdepth.global.dist.txt",
        "final.mosdepth.region.dist.txt",
        "final.mosdepth.summary.txt",
        "final.per-base.bed.gz",
        "final.per-base.bed.gz.csi",
        "final.regions.bed.gz",
        "final.regions.bed.gz.csi",
        "final.phaseblock_data.tsv",
        "final.whatshap_stats.tsv",
        "final.insert_size_metrics.txt",
        "final.insert_size_histogram.pdf"
    ]
    if config["phasing_ground_truth"]:
        inputs.append("final.phasing_stats.txt")
    return inputs


rule multiqc_summarize:
    """Summarizes all reports into one html report. Automatically identifies input/gives output names but by adding
    more input files it controls when snakemake runs the multiqc summary.
    """
    output:
        multiqc_data_dir = directory("multiqc_data"),
        summarized_reports = "multiqc_report.html"
    input:
        get_multiqc_input
    log:
        "multiqc.log"
    shell:
        "multiqc"
        " ."
        " 2> {log}"


def plot_figures_input(wildcards):
    if config["library_type"] == "blr":
        return "final.molecule_stats.filtered.tsv", "barcodes.clstr"
    else:
        return "final.molecule_stats.filtered.tsv"


rule plot_figures:
    """Generate plots for final report."""
    output: directory("figures")
    input: plot_figures_input
    log: "final.plot.log"
    shell:
        "blr plot"
        " {input}"
        " -o {output} 2> {log}"


rule barcode_sort_fastq:
    """Assumes barcode read name is followed by barcode sequence. Required for EMA aligner.
    Exmaple: @ST-E00269:339:H27G2CCX2:7:1102:21186:8060:AAAAAAAATATCTACGCTCA BX:Z:AAAAAAAATATCTACGCTCA
    """
    output:
        fastq = "sorted.{nr}.fastq.gz"
    input:
        fastq = "trimmed.barcoded.{nr}.fastq.gz"
    shell:
        "pigz -cd -p 1 {input.fastq} |"
        " paste - - - - |"
        " sort -t ' ' -k2 |"
        " tr '\t' '\n' |"
        " pigz > {output.fastq}"


rule map_sort_tag:
    """Map reads using the aligner specified in configs. Output is sorted and tagged with barcode."""
    output:
        bam = "initialmapping.bam"
    input:
        r1_fastq = "trimmed.barcoded.1.fastq.gz" if config["read_mapper"] != "ema" else "sorted.1.fastq.gz",
        r2_fastq = "trimmed.barcoded.2.fastq.gz" if config["read_mapper"] != "ema" else "sorted.2.fastq.gz"
    threads: 20
    log:
        map = "read_mapping.log",
        sort = "sorting.log"
    run:
        commands = {
            "bwa":
                "bwa mem"
                " -t {threads}"
                " -R {readgroup}"
                " {config[genome_reference]}"
                " {input.r1_fastq}"
                " {input.r2_fastq}",
            "bowtie2":
                "bowtie2"
                " -p {threads}"
                " --rg-id {readgroup.identifier}"
                " --rg {readgroup.LB}"
                " --rg {readgroup.SM}"
                " --rg {readgroup.PU}"
                " --rg {readgroup.PL}"
                " --reorder"
                " --maxins 2000"
                " -x {config[genome_reference]}"
                " -1 {input.r1_fastq}"
                " -2 {input.r2_fastq}",
            "minimap2":
                "minimap2"
                " -ax sr"
                " -R {readgroup}"
                " -t {threads}"
                " {config[genome_reference]}"
                " {input.r1_fastq}"
                " {input.r2_fastq}",
            "ema":
                "ema align"
                " -1 <(pigz -cd {input.r1_fastq})"
                " -2 <(pigz -cd {input.r2_fastq})"
                " -R {readgroup}"
                " -r {config[genome_reference]}"
                " -t {threads}"
                " -p 10x"
        }
        command = commands[config["read_mapper"]]

        # Use temporary directory for intermediate file if TMPDIR set in environment.
        tmpdir = " -T $TMPDIR" if "TMPDIR" in os.environ else ""

        shell(
            command + " 2> {log.map} | "
            "samtools sort -"
            " -@ {threads}"
            " -o {output.bam}" +
            tmpdir +
            " 2> {log.sort}"
        )


rule make_chunk_beds:
    output:
        expand("chunks/{chunk[0].name}.bed", chunk=chunks)
    run:
        for chunk in chunks:
            with open(f"chunks/{chunk[0].name}.bed", "w") as f:
                for chromosome in chunk:
                    print(chromosome.name, 0, chromosome.length, sep="\t", file=f)


rule split_into_chunks:
    output:
        bam = pipe("chunks/{chunk}.sorted.bam")
    input:
        bam = "initialmapping.bam",
        bai = "initialmapping.bam.bai",
        bed = "chunks/{chunk}.bed",
    shell:
        "samtools view -M -L {input.bed} -o {output.bam} {input.bam}"


rule get_unmapped_reads:
    output:
        bam = "unmapped.bam"
    input:
        bam = "initialmapping.bam"
    log: "unmapped.log"
    shell:
        "samtools view -hf 13 {input.bam} | blr tagbam -o {output.bam} - 2> {log}"


rule tagbam:
    output:
        bam = "{base}.sorted.tag.bam"
    input:
        bam = "{base}.sorted.bam"
    log:
        "{base}.tagbam.log"
    shell:
        "blr tagbam {input.bam} -o {output.bam} 2> {log}"


rule clusterrmdup:
    """Merge cluster duplicates defined as two separate cluster sharing """
    output:
        bam = "{base}.sorted.tag.bcmerge.bam",
        merges = "{base}.barcode-merges.csv"
    input:
        bam = "{base}.sorted.tag.bam"
    log: "{base}.clusterrmdup.log"
    shell:
        "blr clusterrmdup"
        " {input.bam}"
        " {output.merges}"
        " -o {output.bam}"
        " -b {config[cluster_tag]} 2> {log}"


rule mark_duplicates:
    """Mark duplicates within barcodes clusters."""
    output:
        bam = "{base}.sorted.tag.bcmerge.mkdup.bam",
        metrics = "{base}.sorted.tag.bcmerge.mkdup_metrics.txt"
    input:
        bam = "{base}.sorted.tag.bcmerge.bam"
    log: "{base}.mark_duplicates.log"
    shell:
        "picard MarkDuplicates"
        " INPUT={input.bam}"
        " OUTPUT={output.bam}"
        " METRICS_FILE={output.metrics}"
        " READ_ONE_BARCODE_TAG={config[cluster_tag]}"
        " READ_TWO_BARCODE_TAG={config[cluster_tag]}"
        # Running without `USE_JDK_DEFLATER=true` & `USE_JDK_INFLATER=true` causes fatal error. See issue: 
        # https://github.com/broadinstitute/picard/issues/1329
        " USE_JDK_DEFLATER=true"
        " USE_JDK_INFLATER=true"
        " ASSUME_SORTED=true"
        " &> {log}"


rule buildmolecules:
    """Groups reads into molecules depending on their genomic position and barcode"""
    output:
        bam = "{base}.sorted.tag.bcmerge.mkdup.mol.bam",
        stats = "{base}.molecule_stats.tsv"
    input:
        bam = "{base}.sorted.tag.bcmerge.mkdup.bam"
    log: "{base}.buildmolecules.log"
    shell:
        "blr buildmolecules"
        " {input.bam}"
        " -o {output.bam}"
        " --stats-tsv {output.stats}"
        " -m {config[molecule_tag]}"
        " -b {config[cluster_tag]}"
        " --window {config[window_size]}"
        " 2> {log}"


rule concat_molecule_stats:
    output:
        tsv = "final.molecule_stats.tsv"
    input:
        tsv = expand("chunks/{chunk[0].name}.molecule_stats.tsv", chunk=chunks)
    run:
        dfs = list()
        for nr, file in enumerate(input.tsv):
            try:
                df = pd.read_csv(file, sep="\t")
            except pd.errors.EmptyDataError:
                continue
            if not df.empty:
                df["ChunkID"] = nr
                dfs.append(df)


rule get_barcodes_to_filter:
    output:
         tsv = "final.barcodes_filtered_out.tsv"
    input:
         tsv = "final.molecule_stats.tsv"
    run:
        molecules = pd.read_csv(input.tsv, sep="\t")
        molecules_per_barcode = molecules.groupby("Barcode", sort=False)["Barcode"].count()
        barcodes_to_filter = molecules_per_barcode[molecules_per_barcode > config["max_molecules_per_bc"]].index.to_list()
        with open(output.tsv, "w") as file:
            print("\n".join(barcodes_to_filter), file=file)


rule filter_molecule_stats:
    """Remove barcodes that are to be filtered out from stats"""
    output:
        tsv = "final.molecule_stats.filtered.tsv"
    input:
        tsv = "final.molecule_stats.tsv",
        barcodes = "final.barcodes_filtered_out.tsv"
    run:
        molecules = pd.read_csv(input.tsv, sep="\t")
        barcodes_to_filter = pd.read_csv(input.barcodes, sep="\t", names=["Barcode"])
        molecules_filtered = molecules[~molecules["Barcode"].isin(barcodes_to_filter["Barcode"])]
        molecules_filtered.to_csv(output.tsv, sep="\t", index=False)


# filterclusters creates a .bai file directly
ruleorder: filterclusters > index_bam


rule filterclusters:
    """Filter clusters based on number of molecules. Remove duplicates from BAM"""
    output:
        bam = "{base}.sorted.tag.bcmerge.mkdup.mol.filt.bam",
        bai = "{base}.sorted.tag.bcmerge.mkdup.mol.filt.bam.bai"
    input:
        bam = "{base}.sorted.tag.bcmerge.mkdup.mol.bam",
        barcodes = "final.barcodes_filtered_out.tsv"
    log: "{base}.filterclusters.log"
    shell:
        "blr filterclusters"
        " {input.bam}"
        " {input.barcodes}"
        " -m {config[molecule_tag]}"
        " -b {config[cluster_tag]}"
        " -s {config[sequence_tag]}"
        " 2> {log} |"
        " tee {output.bam}"
        " |"
        " samtools index  - {output.bai}"


rule bam_to_fastq:
    """Convert final BAM file to FASTQ files for read 1 and 2"""
    output:
        r1_fastq = "reads.1.final.fastq.gz",
        r2_fastq = "reads.2.final.fastq.gz"
    input:
        bam = "final.bam"
    log: "samtools-fastq.log"
    threads: 20
    shell:
        "samtools fastq"
        " -@ {threads}"
        " -T {config[cluster_tag]},{config[sequence_tag]}"
        " {input.bam}"
        " -1 {output.r1_fastq}"
        " -2 {output.r2_fastq} 2> {log}"


rule recal_base_qual_scores:
    """Recalibrate base calling qualities"""
    output:
        recal_table = "{base}.gatk_BQSR_table.txt"
    input:
        bam = "{base}.sorted.tag.bcmerge.mkdup.mol.filt.bam"
    log: "{base}.recal_base_qual_scores.log"
    shell:
        "gatk --java-options -Xmx{config[heap_space]}g BaseRecalibrator"
        " -R {config[genome_reference]}"
        " --known-sites {config[dbSNP]}"
        " -I {input.bam}"
        " -O {output.recal_table}"
        " 2> {log}"


rule apply_recal:
    """Adjusts BAM with results from gatk base score recalibration"""
    output:
        recal_bam = "{base}.sorted.tag.bcmerge.mkdup.mol.filt.BQSR.bam"
    input:
        bam = "{base}.sorted.tag.bcmerge.mkdup.mol.filt.bam",
        recal_table = "{base}.gatk_BQSR_table.txt"
    log: "{base}.apply_recal.log"
    shell:
        "gatk --java-options -Xmx{config[heap_space]}g ApplyBQSR"
        " --bqsr-recal-file {input.recal_table}"
        " -R {config[genome_reference]}"
        " -I {input.bam}"
        " -O {output.recal_bam}"
        " 2> {log}"


def get_input_calling_bam(wildcards):
    basename = f"{wildcards.base}.sorted.tag.bcmerge.mkdup.mol.filt"
    if config["variant_caller"] == "gatk" and config["BQSR"]:
        basename += ".BQSR"
    return f"{basename}.bam"


rule symlink_calling_bam:
    # The BAM file (with all preprocesing applied) used for calling variants
    output:
        bam = "{base}.calling.bam"
    input:
        bam = get_input_calling_bam
    run:
        symlink_relpath(input.bam, output.bam)


rule index_bam:
    output:
        bai = "{base}.bam.bai"
    input:
        bam = "{base}.bam"
    shell:
        "samtools index"
        " {input.bam}"
        " {output.bai}"


rule compress_vcf:
    output:
        vcf_gz = "{base}.vcf.gz",
    input:
        vcf = "{base}.vcf"
    shell:
        "bgzip -c {input.vcf} > {output.vcf_gz}"


rule index_vcf:
    output:
        "{base}.vcf.gz.tbi"
    input:
        vcf = "{base}.vcf.gz"
    shell:
        "tabix -p vcf {input.vcf}"


rule call_variants:
    """Call variants using the selected variant caller from configs."""
    output:
        vcf = "{base}.variants.called.vcf"
    input:
        bam = "{base}.calling.bam",
        bai = "{base}.calling.bam.bai",
    log: "{base}.call_variants.log"
    threads: 2
    run:
        commands = {
            "freebayes":
                 "freebayes"
                 " -f {config[genome_reference]}"
                 " {input.bam}"
                 " 2> {log}"
                 " | "
                 # Filter out variants with allele count above 0 as these are detected as polyploid by hapcut2
                 "vcffilter"
                 " -f"
                 " 'AC > 0'"
                 " 1> {output.vcf} 2>> {log}",
            "bcftools":
                "bcftools mpileup"
                " -f {config[genome_reference]}"
                " {input.bam}"
                " --threads {threads}"
                " 2> {log}"
                " | "
                "bcftools call"
                " -m"
                " -v"
                " -O v"
                " --ploidy GRCh38"
                " --threads {threads}"
                " -o {output.vcf} 2>> {log}",
            "gatk":
                "gatk --java-options '-Xmx{config[heap_space]}g -XX:ParallelGCThreads={threads}' HaplotypeCaller"
                " -R {config[genome_reference]}"
                " -I {input.bam}"
                " -O {output.vcf} 2> {log}",
            # This option is currently not included in the pipeline documentation, but it will run on any system which
            # - runs Linux (it's not maintained for any other system)
            # - has deepvariant installed (e.g. through conda)
            # TODO: Add a conditional installation of deepvariant for Linux systems and add to pipeline docs.
            "deepvariant":
                "deepvariant"
	            " --model_type=WGS"
	            " --output_vcf={output.vcf}"
	            " --reads={input.bam}"
	            " --ref={config[genome_reference]}"
                }
        shell(commands[config["variant_caller"]])


def get_phase_input_vcf(wildcards):
    if config["reference_variants"]:
        return {
            "vcf": config["reference_variants"],
            "bed": f"chunks/{wildcards.chunk}.bed",
        }
    else:
        return {"vcf": f"chunks/{wildcards.chunk}.variants.called.vcf"}


rule split_or_symlink_phase_input_vcf:
    output:
        vcf = "chunks/{chunk}.phaseinput.vcf"
    input:
        unpack(get_phase_input_vcf)
    run:
        if config["reference_variants"]:
            # TODO
            # bcftools view -T is possibly a bit inefficient (use an indexed VCF instead)
            shell("bcftools view -o {output.vcf} -T {input.bed} {input.vcf}")
        else:
            symlink_relpath(input.vcf, output.vcf)


rule merge_bams:
    output:
        bam = "final.{phased,(phased.|)}bam"
    input:
        bam = expand("chunks/{chunk[0].name}.calling.{{phased}}bam", chunk=chunks),
        bam_unmapped = "unmapped.bam"
    shell:
        "samtools cat -o {output.bam} {input.bam} {input.bam_unmapped}"


rule concat_phased_vcfs:
    output:
        vcf = "final.phased.vcf"
    input:
        vcf = expand("chunks/{chunk[0].name}.calling.phased.vcf", chunk=chunks)
    shell:
        "bcftools concat -o {output.vcf} {input.vcf}"


rule concat_called_vcfs:
    output:
        vcf = "called.vcf"
    input:
        vcf = expand("chunks/{chunk[0].name}.variants.called.vcf", chunk=chunks)
    shell:
        "bcftools concat -o {output.vcf} {input.vcf}"


rule mosdepth:
    """Calculate depth stats"""
    output:
        "final.mosdepth.global.dist.txt",
        "final.mosdepth.region.dist.txt",
        "final.mosdepth.summary.txt",
        "final.per-base.bed.gz",
        "final.per-base.bed.gz.csi",
        "final.regions.bed.gz",
        "final.regions.bed.gz.csi"
    input:
        bam = "final.bam",
        bai = "final.bam.bai"
    params:
        prefix = "final",
        bin_size = config["coverage_window_size"]
    threads: 20
    shell:
        "mosdepth"
        " --threads {threads}"
        " --by {params.bin_size}"
        " {params.prefix}"
        " {input.bam}"


rule aggregate_phaseblock_data:
    output:
        tsv = "final.phaseblock_data.tsv"
    input:
        phase = expand("chunks/{chunk[0].name}.calling.phase", chunk=chunks)
    run:
        data = list()
        with open(output.tsv, "w") as output_tsv:
            print("Variants spanned", "Variants phased", "Length", "Fragments", sep="\t", file=output_tsv)
            for input_file in input.phase:
                with open(input_file) as file:
                    for p in parse_phaseblocks(file):
                        print(p.snv_span, p.phased_snvs, p.length, p.fragments, sep="\t", file=output_tsv)

rule whatshap_stats:
    """Calculate phaseing stats related to variants and phaseblocks."""
    output:
        tsv = "final.whatshap_stats.tsv"
    input:
        phased_vcf = "final.phased.vcf"
    log: "whatshap_stats.log"
    shell:
         "whatshap stats"
         " --tsv {output.tsv}"
         " {input.phased_vcf} > {log}"


rule collect_insert_size_metrics:
    """Collect insert size metrics for BAM file."""
    output:
        metrics = "final.insert_size_metrics.txt",
        hist = "final.insert_size_histogram.pdf",
    input:
        bam = "final.bam"
    log: "collect_insert_size_metrics.log"
    shell:
        "picard CollectInsertSizeMetrics"
        " I={input.bam}"
        " O={output.metrics}"
        " H={output.hist} 2> {log}"
