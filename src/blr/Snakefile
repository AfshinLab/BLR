import os
import pickle
from pathlib import Path

import pandas as pd
from snakemake.utils import validate
from snakemake.logging import Logger
from snakemake.exceptions import WorkflowError

from blr.utils import ReadGroup, generate_chunks, symlink_relpath, parse_phaseblocks, parse_filters, tempif
from blr.cli.find_clusterdups import UnionFind

configfile: "blr.yaml"
validate(config, "config.schema.yaml")

if config["BQSR"] and config["known_sites"] is None:
    raise WorkflowError("When BQSR is True, 'known_sites' needs to be set. Use: "
                        "'$ blr config -s known_sites /path/to/sites.vcf.gz'")

# TODO remove in next version
if config["library_type"] == "blr":
    Logger().warning("DEPRECIATION: The 'blr' option for library type will soon be replaced with 'dbs'. "
                     "Replace 'blr' with 'dbs' for library_type in the future.")


# Hidden config options, starts with '_'
config["_ema_bins_dir"] = Path("fastq_bins")
if config["fastq_bins"] > 1:
    config["_fastq_bin_nrs"] = [str(i).zfill(3) for i in range(config['fastq_bins'])]

# Import rules for trimming fastq files.
if config["library_type"] in {"blr", "dbs"}:  # TODO Remove blr
    include: "rules/trim_dbs.smk"
elif config["library_type"] == "10x":
    include: "rules/trim_10x.smk"
elif config["library_type"] == "stlfr":
    include: "rules/trim_stlfr.smk"
elif config["library_type"] == "tellseq":
    include: "rules/trim_tellseq.smk"

# Import rules for phasing
include: "rules/phasing.smk"

# Create read group string to tag reads.
platform = "DNBSEQ" if config["library_type"] == "stlfr" else "ILLUMINA"
readgroup = ReadGroup(identifier=config["sample_nr"], library=config['library_type'], sample="20", platfrom_unit="unit1",
                      platform=platform)


# For parallelization, we split the initial mapped BAM file into non-overlapping "chunks",
# which are computed from the FASTA index file.
chunks = generate_chunks(reference=config["genome_reference"],
                         size=config["chunk_size"],
                         phasing_contigs_string=config["phasing_contigs"],
                         contigs_skipped=config["contigs_skipped"])

skip_tagbam = (config["library_type"] == "10x" and config["read_mapper"] == "ema") or \
              config["read_mapper"] == "lariat"

bcmerge = ".bcmerge" if not config["skip_bcmerge"] else ""

# Duplicates and molecules are already marked by ema but need to re-marked if barcodes are merged.
mkdup = ".mkdup" if config["read_mapper"] != "ema" or bcmerge else ""
mol = ".mol" if config["read_mapper"] != "ema" or bcmerge else ""
filt = ".filt" if config["max_molecules_per_bc"] > 0 else ""


# Extension from indexed genome references
BWA_INDEX_EXT = [".amb", ".ann", ".bwt", ".pac", ".sa"]
BOWTIE2_INDEX_EXT = [".1.bt2", ".2.bt2", ".3.bt2", ".4.bt2", ".rev.1.bt2", ".rev.1.bt2"]


def final_input(wildcards):
    files = [
        "multiqc_report.html",
        "final.bam",
        "final.bam.bai",
        "final.phased.cram",
        "final.phased.cram.crai",
        "final.phased.vcf.gz",
        "final.phased.vcf.gz.tbi",
        "final.naibr_sv_calls.bedpe",
        "final.naibr_sv_calls.vcf.gz",
        "final.naibr_sv_calls.vcf.gz.tbi",
        "final.sv_sizes.tsv"
    ]
    if not config["reference_variants"]:
        files.extend([
            "called.vcf.gz",
            "called.vcf.gz.tbi"
        ])
        if config["filter_variants"]:
            files.extend([
                "called.filtered.vcf.gz",
                "called.filtered.vcf.gz.tbi"
            ])
    return files


rule final:
   input: final_input


rule fastqc:
    """Creates FastQC reports from trimmed read files. Output names are automatically set by FastQC."""
    output:
        qc = "trimmed.barcoded.{nr,[12]}_fastqc.html",
        zip = "trimmed.barcoded.{nr,[12]}_fastqc.zip",
    input:
        reads = "trimmed.barcoded.{nr}.fastq.gz",
    log: "trimmed.barcoded.{nr,[12]}_fastqc.html.log"
    threads: 2  # Fix java.lang.OutOfMemoryError (https://github.com/s-andrews/FastQC/issues/24)
    shell:
        "fastqc"
        " {input.reads}"
        " -t {threads}"
        " &> {log}"


def get_multiqc_input(wildcards):
    inputs = [
        "final.mosdepth.global.dist.txt",
        "final.mosdepth.summary.txt",
        "final.phaseblock_data.tsv",
        "final.whatshap_stats.tsv",
        "final.insert_size_metrics.txt",
        "final.insert_size_histogram.pdf",
        "final.phasing_stats.txt",
        "final.duplicate_metrics.txt",
        "final.molecule_lengths.tsv",
        "final.alignment_summary_metrics.txt",
        "final.gc_bias.detail_metrics.txt",
        "final.gc_bias.summary_metrics.txt",
        "final.samtools_stats.txt",
        "final.sv_sizes.tsv",
        "final.molecule_stats.txt",
        "blr_mqc.yaml",
        "versions_mqc.yaml",
        "ideogram_mqc.html",
    ]
    # Lariat uses interleaved FASTQs with a special format that cannot be handled by FastQC
    if config["read_mapper"] != "lariat":
        inputs.extend(["trimmed.barcoded.1_fastqc.html", "trimmed.barcoded.2_fastqc.html"])

    if config["library_type"] in {"dbs", "blr", "tellseq"}:
        inputs.append("final.barcode_stats.txt")
    return inputs


rule multiqc:
    """Summarizes all stats into a MultiQC HTML report."""
    output:
        multiqc_data_dir = directory("multiqc_data"),
        summarized_reports = "multiqc_report.html"
    input:
        get_multiqc_input
    log:
        "multiqc_report.html.log"
    shell:
        "multiqc . 2> {log}"


rule report_configs:
    """Translate configs to HTML to include in MultiQC report"""
    input:
        yaml = "blr.yaml"
    output:
        yaml = "blr_mqc.yaml"
    params:
        configs = config
    script:
        "scripts/report_configs.py"


rule get_versions:
    '''Get software versions for MultiQC report'''
    output:
        yaml = "versions.yaml"
    run:
        # TODO add deepvariant
        shell("echo 'snakemake:' $(snakemake --version) >> {output.yaml}")
        shell("echo 'blr:' $(blr --version) >> {output.yaml}")
        shell("echo 'python:' $(python --version 2>&1 | sed 's|^Python ||') >> {output.yaml}")
        shell("echo 'cutadapt:' $(cutadapt --version) >> {output.yaml}")
        shell("echo 'starcode:' $(starcode --version 2>&1 | sed 's|^starcode-v||;s| .*$||') >> {output.yaml}")
        shell("echo 'fastqc:' $(fastqc --version 2>&1 | sed 's|^FastQC v||') >> {output.yaml}")
        shell("echo 'multiqc:' $(multiqc --version | sed 's|^multiqc, version ||') >> {output.yaml}")
        shell("echo 'ema:' $(ema 2>&1 | head -1 | sed 's|^EMA version ||') >> {output.yaml}")
        shell("echo 'bwa:' $(bwa 2>&1 | grep ^Version | sed 's|^Version: ||') >> {output.yaml}")
        shell("echo 'samtools:' $(samtools --version 2>&1 | grep ^samtools | sed 's|^.*samtools ||') >> {output.yaml}")
        shell("echo 'picard:' $( picard MarkDuplicates --version 2>&1 | sed 's|^Version:||') >> {output.yaml}")
        shell("echo 'bowtie2:' $(bowtie2 --version 2>&1 | head -1 | sed 's|^.* version ||') >> {output.yaml}")
        shell("echo 'minimap2:' $(minimap2 --version) >> {output.yaml}")
        shell("echo 'lariat:' $(command -v lariat >/dev/null && lariat 2>&1 | head -1 | sed 's|^Starting lariat. Version: ||' | tr -d \\') >> {output.yaml}")
        shell("echo 'gatk:' $(gatk --version 2>&1 | grep '^The' | sed 's|^The Genome Analysis Toolkit (GATK) v||') >> {output.yaml}")
        shell("echo 'freebayes:' $(freebayes --version 2>&1 | sed 's|^version:.*v||') >> {output.yaml}")
        shell("echo 'vcflib:' $(vcffilter 2>&1 | head -1 | sed 's|^vcflib ||;s| filter.*$||') >> {output.yaml}")
        shell("echo 'bcftools:' $(bcftools --version 2>&1 | head -1 | sed 's|^bcftools ||') >> {output.yaml}")
        shell("echo 'mosdepth:' $(mosdepth --version 2>&1 | sed 's|^mosdepth ||') >> {output.yaml}")
        shell("echo 'whatshap:' $(whatshap --version | sed 's|^whatshap ||') >> {output.yaml}")


rule report_versions:
    '''Format software versions for MultiQC report'''
    input:
        yaml = "versions.yaml"
    output:
        yaml = "versions_mqc.yaml"
    script:
        "scripts/report_versions.py"


rule generate_ideogram:
    """Generate phaseblock ideogram over all phased chromomsomes"""
    output:
        html = "ideogram_mqc.html"
    input:
        phased_vcf = "final.phased.vcf.gz"
    params:
        assembly = config["ideogram_assembly"]
    script:
        "scripts/ideogram_html.py"


rule molecule_stats:
    """Generate molecule stats for MultiQC report."""
    output:
        txt = "final.molecule_stats.txt"
    input:
        tsv = "final.molecule_stats.filtered.tsv"
    script:
        "scripts/molecule_stats.py"


rule barcode_stats:
    """Generate barcode stats for MultiQC report."""
    output:
        txt = "final.barcode_stats.txt"
    input:
        clstr = ancient("barcodes.clstr.gz")
    script:
        "scripts/barcode_stats.py"


rule map_sort_ema_bins:
    """Map binned FASTQs using EMA and sort output"""
    output:
        bam = temp("{dir}/ema-bin-{bin_nr,\d+}.bam")
    input:
        interleaved_fastq = "{dir}/ema-bin-{bin_nr}",
        reference = multiext(config['genome_reference'], *BWA_INDEX_EXT)
    threads: 4
    log:
        map = "{dir}/ema-bin-{bin_nr}.bam.mapping.log",
        sort = "{dir}/ema-bin-{bin_nr}.bam.sorting.log"
    params:
        tmpdir = " -T $TMPDIR" if "TMPDIR" in os.environ else "",
        # Binned FASTQs from 10x are processed using `ema preproc` and have a different format than DBS and Tellseq
        # See: https://github.com/arshajii/ema#input-formats
        input_mode = "-s" if config["library_type"] in ["10x", "stlfr"] else "-1",
        readgroup = readgroup,
        sample_nr = config["sample_nr"],
        reference = config["genome_reference"],
        heap_space = str(config['heap_space']) + "G",
        density_optimization = " -d" if config["ema_optimization"] else "",
    shell:
        "ema align"
        " -t {threads}"
        "{params.density_optimization}"
        " -p 10x"
        " -R {params.readgroup}"
        " -i {params.sample_nr}"
        " -r {params.reference}"
        " {params.input_mode} {input.interleaved_fastq}"
        " 2>> {log.map}"
        " |"
        " samtools sort "
        " -@ {threads}"
        " -m {params.heap_space}"
        " -O bam"
        " -l 0"
        "{params.tmpdir}"
        " -o {output.bam}"
        " - 2> {log.sort}"


rule map_sort_nobc:
    """Only for read_mapper=ema. Map and Sort non barcoded reads with BWA"""
    output:
        bam = temp("initialmapping_nobc.bam")
    input:
        r1_fastq = "trimmed.non_barcoded.1.fastq.gz",
        r2_fastq = "trimmed.non_barcoded.2.fastq.gz",
        reference = multiext(config['genome_reference'], *BWA_INDEX_EXT)
    threads: 4
    log:
        map = "initialmapping_nobc.bam.mapping.log",
        sort = "initialmapping_nobc.bam.sorting.log"
    params:
        tmpdir = " -T $TMPDIR" if "TMPDIR" in os.environ else "",
        readgroup = readgroup,
        reference = config["genome_reference"],
        heap_space = str(config['heap_space']) + "G"
    shell:
        "bwa mem"
        " -t {threads}"
        " -R {params.readgroup}"
        " {params.reference}"
        " {input.r1_fastq}"
        " {input.r2_fastq}"
        " 2> {log.map}"
        " |"
        " samtools sort "
        " -@ {threads}"
        " -m {params.heap_space}"
        " -O bam"
        " -l 0"
        "{params.tmpdir}"
        " -o {output.bam}"
        " - 2> {log.sort}"


rule mark_duplicates_nobc:
    """Mark duplicates for non barcoded reads."""
    output:
        bam = temp("initialmapping_nobc.mkdup.bam"),
        metrics = "initialmapping_nobc.mkdup_metrics.txt"
    input:
        bam = "initialmapping_nobc.bam"
    log: "initialmapping_nobc.mkdup.bam.log"
    params:
        # 250_000 per Gb is recommended according to https://sourceforge.net/p/picard/wiki/Main_Page/
        max_records_in_ram = int(250_000 * config['heap_space']),
        java_args = f"-Xmx{config['heap_space']}g"
    shell:
        "picard {params.java_args} MarkDuplicates"
        " INPUT={input.bam}"
        " OUTPUT={output.bam}"
        " METRICS_FILE={output.metrics}"
        # Running without `USE_JDK_DEFLATER=true` & `USE_JDK_INFLATER=true` causes fatal error. See issue: 
        # https://github.com/broadinstitute/picard/issues/1329
        " USE_JDK_DEFLATER=true"
        " USE_JDK_INFLATER=true"
        " ASSUME_SORTED=true"
        " MAX_RECORDS_IN_RAM={params.max_records_in_ram}"
        " VALIDATION_STRINGENCY=LENIENT"
        " COMPRESSION_LEVEL=0"
        " &> {log}"


if config["read_mapper"] == "ema" and config["fastq_bins"] > 1:
    ruleorder: merge_mapped_ema_bins > map_sort
else:
    ruleorder: map_sort > merge_mapped_ema_bins


rule merge_mapped_ema_bins:
    """Merge mapped and sorted bins from EMA with non-barcoded mapped reads. Output is sorted."""
    output:
        bam = "initialmapping.bam",
        mapping_log = "initialmapping.bam.mapping.log",
        sorting_log = "initialmapping.bam.sorting.log",
    input:
        bams = expand(config['_ema_bins_dir'] / "ema-bin-{nr}.bam", nr=config["_fastq_bin_nrs"]),
        bais = expand(config['_ema_bins_dir'] / "ema-bin-{nr}.bam.bai", nr=config["_fastq_bin_nrs"]),
        # If we are goning to mark duplicates after merging there is no need to do if before for the nobc reads.
        bam_nobc = "initialmapping_nobc.bam" if mkdup else "initialmapping_nobc.mkdup.bam",
        bais_nobc = "initialmapping_nobc.bam.bai" if mkdup else "initialmapping_nobc.mkdup.bam.bai"
    threads: 20
    params:
        logs_dir = config["_ema_bins_dir"]
    shell:
        "samtools merge -cp -@ {threads} {output.bam} {input.bams} {input.bam_nobc}"
        " &&"
        " cat {params.logs_dir}/*.mapping.log > {output.mapping_log}" 
        " &&"
        " cat {params.logs_dir}/*.sorting.log > {output.sorting_log}"


rule map_sort:
    """Map reads using the aligner specified in configs. Output is sorted."""
    output:
        bam = "initialmapping.bam"
    input:
        r1_fastq = "trimmed.barcoded.1.fastq.gz",
        r2_fastq = "trimmed.barcoded.2.fastq.gz",
        ref = multiext(config['genome_reference'],
                       *(BOWTIE2_INDEX_EXT
                       if config["read_mapper"] == "bowtie2" else
                       BWA_INDEX_EXT))
    threads: 20
    log:
        map = "initialmapping.bam.mapping.log",
        sort = "initialmapping.bam.sorting.log"
    params:
        tmpdir = " -T $TMPDIR" if "TMPDIR" in os.environ else "",
        reference = config["genome_reference"],
        readgroup = readgroup,
        sample_nr = config["sample_nr"],
        mapper = config["read_mapper"],
        heap_space = str(config['heap_space']) + "G",
        density_optimization = " -d" if config["ema_optimization"] else "",
    run:
        command = {
            "bwa":
                "bwa mem"
                " -t {threads}"
                " -R {params.readgroup}"
                " {params.reference}"
                " {input.r1_fastq}"
                " {input.r2_fastq}",
            "bowtie2":
                "bowtie2"
                " -p {threads}"
                " --rg-id {params.readgroup.identifier}"
                " --rg {params.readgroup.LB}"
                " --rg {params.readgroup.SM}"
                " --rg {params.readgroup.PU}"
                " --rg {params.readgroup.PL}"
                " --maxins 2000"
                " -x {params.reference}"
                " -1 {input.r1_fastq}"
                " -2 {input.r2_fastq}",
            "minimap2":
                "minimap2"
                " -ax sr"
                " -R {params.readgroup}"
                " -t {threads}"
                " {params.reference}"
                " {input.r1_fastq}"
                " {input.r2_fastq}",
            "ema":
                "ema align"
                " -1 <(pigz -cd {input.r1_fastq})"
                " -2 <(pigz -cd {input.r2_fastq})"
                " -R {params.readgroup}"
                " -r {params.reference}"
                " -t {threads}"
                " -i {params.sample_nr}"
                " -p 10x"
                "{params.density_optimization}",
            "lariat":
                # Experimental addition!
                # Lariat creates a output directory with three files:
                #  - 000000-chrA_0000000000_pos_bucketed.bam -> Mapped barcode tagged reads for chunk (multiple if run
                #    on several chunks)
                #  - ZZZ_unmapped_pos_bucketed.bam --> Unmapped reads
                #  - bc_sorted_bam.bam --> Concatenated BAM with all reads.
                # Only the `bc_sorted_bam.bam` is used currently and the rest deleted on step completion. Lariat does not
                # add read groups (or I cannot figure out how) so this is done separately.
                "mkdir -p lariat_tmp"
                " && "
                "trap 'rm -rf lariat_tmp' EXIT"
                " && "
                "lariat"
                " -reads={input.r1_fastq}"
                " -genome={params.reference}"
                " -threads={threads}"
                " -trim_length=0"
                " -first_chunk=True"
                " -output=lariat_tmp"
                " -sample_id={params.readgroup.sample} &> {log.map}"
                " && "  
                "samtools addreplacerg"  
                " -r {params.readgroup.ID}"
                " -r {params.readgroup.LB}"
                " -r {params.readgroup.SM}"
                " -r {params.readgroup.PL}"
                " -r {params.readgroup.PU}"
                " lariat_tmp/bc_sorted_bam.bam"
        }[params.mapper]

        shell(
            f"{command} 2>> {log.map} | "
            "samtools sort -"
            " -@ {threads}"
            " -m {params.heap_space}"
            " -o {output.bam}"
            "{params.tmpdir}"
            " 2> {log.sort}"
        )


rule make_chunk_beds:
    output:
        expand("chunks/{chunk[0].name}.bed", chunk=chunks["all"])
    run:
        for chunk in chunks["all"]:
            with open(f"chunks/{chunk[0].name}.bed", "w") as f:
                for chromosome in chunk:
                    print(chromosome.name, 0, chromosome.length, sep="\t", file=f)


rule split_into_chunks:
    output:
        bam = tempif("chunks/{chunk}.sorted.tag.bam", filt or bcmerge) if skip_tagbam else pipe("chunks/{chunk}.sorted.bam")
    input:
        bam = "initialmapping.bam",
        bai = "initialmapping.bam.bai",
        bed = "chunks/{chunk}.bed",
    shell:
        "samtools view -M -L {input.bed} -o {output.bam} {input.bam}"


rule get_unmapped_reads:
    output:
        bam = temp("unmapped.bam")
    input:
        bam = "initialmapping.bam",
        bai = "initialmapping.bam.bai"
    log: "unmapped.bam.log"
    params:
        tag = ">" if skip_tagbam else f"| blr tagbam - -s {config['sample_nr']}"
                                      f" -b {config['cluster_tag']} -o",
        outtype = "-bh" if skip_tagbam else "-h"
    shell:
        "samtools view {params.outtype} {input.bam} '*' {params.tag} {output.bam} 2> {log}"


rule tagbam:
    output:
        bam = tempif("chunks/{base}.tag.bam", filt or bcmerge)
    input:
        bam = "chunks/{base}.bam"
    log:
        "chunks/{base}.tag.bam.log"
    params:
        mapper = config["read_mapper"],
        sample_nr = config["sample_nr"],
        barcode_tag = config["cluster_tag"],
    shell:
        "blr tagbam "
        " -o {output.bam}"
        " --sample-nr {params.sample_nr}"
        " --barcode-tag {params.barcode_tag}"
        " {input.bam} 2> {log}"


rule find_clusterdups:
    """Find cluster duplicates defined as two separate barcodes sharing duplicate read pair"""
    output:
        pickle = temporary("chunks/{base}.clusterdups.pickle")
    input:
        bam = "chunks/{base}.bam"
    log: "chunks/{base}.clusterdups.pickle.log"
    params:
        min_mapq = config["min_mapq"],
        library_type = config["library_type"],
        window = config["window_size"],
        barcode_tag = config["cluster_tag"],
    shell:
        "blr find_clusterdups"
        " {input.bam}"
        " --output-pickle {output.pickle}"
        " --min-mapq {params.min_mapq}"
        " --library-type {params.library_type}"
        " --window {params.window}"
        " --quantile-threshold 0.99"
        " -b {params.barcode_tag} 2> {log}"


rule get_barcode_merges:
    """Merge graphs of connected barcodes from all chunks to get CSV of barcodes to merge."""
    output:
        merges = "final.barcode-merges.csv"
    input:
        pickle = expand("chunks/{chunk[0].name}.sorted.tag.clusterdups.pickle", chunk=chunks["primary"])
    run:
        uf = UnionFind()
        for filepath in input.pickle:
            with open(filepath, "rb") as file:
                uf_file = UnionFind.from_dict(pickle.load(file))
                uf.update(uf_file)

        with open(output.merges, 'w') as file:
            for old_barcode, new_barcode in uf.items():
                if old_barcode != new_barcode:
                    print(f"{old_barcode},{new_barcode}", file=file)


rule merge_clusterdups:
    """Merge cluster duplicates defined as two separate cluster sharing """
    output:
        bam = temp("chunks/{base}.bcmerge.bam"),
    input:
        bam = "chunks/{base}.bam",
        merges = "final.barcode-merges.csv"
    log: "chunks/{base}.bcmerge.bam.log"
    params:
        barcode_tag = config["cluster_tag"],
    shell:
        "blr merge_clusterdups"
        " {input.bam}"
        " {input.merges}"
        " -o {output.bam}"
        " -b {params.barcode_tag} 2> {log}"


rule mark_duplicates:
    """Mark duplicates within barcodes clusters."""
    output:
        bam = temp("chunks/{base}.mkdup.bam"),
        metrics = "chunks/{base}.mkdup_metrics.txt"
    input:
        bam = "chunks/{base}.bam"
    log: "chunks/{base}.mkdup.bam.log"
    params:
        # 250_000 per Gb is recommended according to https://sourceforge.net/p/picard/wiki/Main_Page/
        max_records_in_ram = int(250_000 * config['heap_space']),
        java_args = f"-Xmx{config['heap_space']}g",
        barcode_tag = config["cluster_tag"],
    shell:
        "picard {params.java_args} MarkDuplicates"
        " INPUT={input.bam}"
        " OUTPUT={output.bam}"
        " METRICS_FILE={output.metrics}"
        " READ_ONE_BARCODE_TAG={params.barcode_tag}"
        " READ_TWO_BARCODE_TAG={params.barcode_tag}"
        # Running without `USE_JDK_DEFLATER=true` & `USE_JDK_INFLATER=true` causes fatal error. See issue: 
        # https://github.com/broadinstitute/picard/issues/1329
        " USE_JDK_DEFLATER=true"
        " USE_JDK_INFLATER=true"
        " ASSUME_SORTED=true"
        " MAX_RECORDS_IN_RAM={params.max_records_in_ram}"
        " VALIDATION_STRINGENCY=LENIENT"
        " COMPRESSION_LEVEL=0"
        " &> {log}"


if mol:
    ruleorder: buildmolecules > readmolecules
else:
    ruleorder: readmolecules > buildmolecules


rule buildmolecules:
    """Groups reads into molecules depending on their genomic position and barcode"""
    output:
        bam = tempif("chunks/{base}.mol.bam", filt),
        stats = temp("chunks/{base}.molecule_stats.tsv")
    input:
        bam = "chunks/{base}.bam"
    log: "chunks/{base}.mol.bam.log"
    params:
        barcode_tag = config["cluster_tag"],
        molecule_tag = config["molecule_tag"],
        min_mapq = config["min_mapq"],
        library_type = config["library_type"],
        window = config["window_size"],
    shell:
        "blr buildmolecules"
        " {input.bam}"
        " -o {output.bam}"
        " --stats-tsv {output.stats}"
        " -m {params.molecule_tag}"
        " -b {params.barcode_tag}"
        " --window {params.window}"
        " --min-mapq {params.min_mapq}"
        " --library-type {params.library_type}"
        " 2> {log}"


rule readmolecules:
    """Read molecule information for BAM with set molecule_tag"""
    output:
        stats = temp("chunks/{base}.molecule_stats.tsv")
    input:
        bam = "chunks/{base}.bam"
    log: "chunks/{base}.molecule_stats.tsv.log"
    params:
        barcode_tag = config["cluster_tag"],
        molecule_tag = config["molecule_tag"],
        min_mapq = config["min_mapq"],
        library_type = config["library_type"],
    shell:
        "blr readmolecules"
        " {input.bam}"
        " --output-tsv {output.stats}"
        " -m {params.molecule_tag}"
        " -b {params.barcode_tag}"
        " --min-mapq {params.min_mapq}"
        " --library-type {params.library_type}"
        " 2> {log}"


rule concat_molecule_stats:
    output:
        tsv = "final.molecule_stats.tsv"
    input:
        tsv = [f"chunks/{chunk[0].name}.sorted.tag{bcmerge}{mkdup}.molecule_stats.tsv" for chunk in chunks["primary"]]
    run:
        dfs = list()
        for nr, file in enumerate(input.tsv):
            try:
                df = pd.read_csv(file, sep="\t")
            except pd.errors.EmptyDataError:
                continue

            df["ChunkID"] = nr
            dfs.append(df)

        concat = pd.concat(dfs, ignore_index=True)
        concat.to_csv(output.tsv, sep="\t", index=False)


rule get_barcodes_to_filter:
    output:
         tsv = "final.barcodes_filtered_out.tsv"
    input:
         tsv = "final.molecule_stats.tsv"
    params:
        threshold = config["max_molecules_per_bc"]
    run:
        if params.threshold > 0:
            molecules = pd.read_csv(input.tsv, sep="\t")
            molecules_per_barcode = molecules.groupby("Barcode", sort=False)["Barcode"].count()
            barcodes_to_filter = molecules_per_barcode[molecules_per_barcode > params.threshold].index.to_list()
            with open(output.tsv, "w") as file:
                print("\n".join(barcodes_to_filter), file=file)
        else:
            shell("touch {output.tsv}")


rule filter_molecule_stats:
    """Remove barcodes that are to be filtered out from stats"""
    output:
        tsv = "final.molecule_stats.filtered.tsv"
    input:
        tsv = "final.molecule_stats.tsv",
        barcodes = "final.barcodes_filtered_out.tsv"
    run:
        molecules = pd.read_csv(input.tsv, sep="\t")
        barcodes_to_filter = pd.read_csv(input.barcodes, sep="\t", names=["Barcode"])
        if len(barcodes_to_filter) > 0:
            molecules_filtered = molecules[~molecules["Barcode"].isin(barcodes_to_filter["Barcode"])]
            molecules_filtered.to_csv(output.tsv, sep="\t", index=False)
        else:
            symlink_relpath(input.tsv, output.tsv)

# filterclusters creates a .bai file directly
ruleorder: filterclusters > index_bam


rule filterclusters:
    """Filter clusters based on number of molecules. Remove duplicates from BAM"""
    output:
        bam = "chunks/{base}.filt.bam",
        bai = "chunks/{base}.filt.bam.bai"
    input:
        bam = "chunks/{base}.bam",
        barcodes = "final.barcodes_filtered_out.tsv"
    log: "chunks/{base}.filt.bam.log"
    params:
        barcode_tag = config["cluster_tag"],
        molecule_tag = config["molecule_tag"],
    shell:
        "blr filterclusters"
        " {input.bam}"
        " {input.barcodes}"
        " -m {params.molecule_tag}"
        " -b {params.barcode_tag}"
        " 2> {log} |"
        " tee {output.bam}"
        " |"
        " samtools index  - {output.bai}"


rule bam_to_fastq:
    """Convert final BAM file to FASTQ files for read 1 and 2"""
    output:
        r1_fastq = "reads.1.final.fastq.gz",
        r2_fastq = "reads.2.final.fastq.gz"
    input:
        bam = "final.bam"
    log: "reads.1.final.fastq.gz.log"
    threads: 20
    params:
        tags = ",".join([config["cluster_tag"], config["sequence_tag"]])
    shell:
        "samtools fastq"
        " -@ {threads}"
        " -T {params.tags}"
        " {input.bam}"
        " -1 {output.r1_fastq}"
        " -2 {output.r2_fastq} 2> {log}"


rule recal_base_qual_scores:
    """Recalibrate base calling qualities"""
    output:
        recal_table = "chunks/{base}.bsqr_table.txt"
    input:
        bam = "chunks/{base}.bam"
    log: "chunks/{base}.bsqr_table.txt.log"
    params:
        java_args = f"-Xmx{config['heap_space']}g",
        reference = config["genome_reference"],
        knowns_sites = ' '.join([f"--known-sites {s}" for s in config["known_sites"].split(",")])
                       if config["known_sites"] is not None else ''
    shell:
        "gatk --java-options {params.java_args} BaseRecalibrator"
        " -R {params.reference}"
        " {params.knowns_sites}"
        " -I {input.bam}"
        " -O {output.recal_table}"
        " 2> {log}"


rule apply_recal:
    """Adjusts BAM with results from gatk base score recalibration"""
    output:
        recal_bam = "chunks/{base}.BQSR.bam"
    input:
        bam = "chunks/{base}.bam",
        recal_table = "chunks/{base}.bsqr_table.txt"
    log: "chunks/{base}.BQSR.bam.log"
    params:
        java_args = f"-Xmx{config['heap_space']}g",
        reference = config["genome_reference"],
    shell:
        "gatk --java-options {params.java_args} ApplyBQSR"
        " --bqsr-recal-file {input.recal_table}"
        " -R {params.reference}"
        " -I {input.bam}"
        " -O {output.recal_bam}"
        " 2> {log}"


def get_input_calling_bam(wildcards):
    basename = f"chunks/{wildcards.base}.sorted.tag{bcmerge}{mkdup}{mol}{filt}"
    if config["variant_caller"] == "gatk" and config["BQSR"]:
        basename += ".BQSR"
    return f"{basename}.bam"


rule symlink_calling_bam:
    # The BAM file (with all preprocesing applied) used for calling variants
    output:
        bam = "chunks/{base}.calling.bam"
    input:
        bam = get_input_calling_bam
    run:
        symlink_relpath(input.bam, output.bam)


rule index_bam:
    output:
        bai = "{base}.bam.bai"
    input:
        bam = "{base}.bam"
    shell:
        "samtools index {input.bam} {output.bai}"


rule index_cram:
    output:
        crai = "{base}.cram.crai"
    input:
        cram = "{base}.cram"
    shell:
        "samtools index {input.cram} {output.crai}"


rule compress_vcf:
    output:
        vcf_gz = "{base}.vcf.gz",
    input:
        vcf = "{base}.vcf"
    shell:
        "bgzip -c {input.vcf} > {output.vcf_gz}"


rule index_vcf:
    output:
        "{base}.vcf.gz.tbi"
    input:
        vcf = "{base}.vcf.gz"
    shell:
        "tabix -p vcf {input.vcf}"


rule call_variants:
    """Call variants using the selected variant caller from configs."""
    output:
        vcf = "chunks/{base}.variants.called.vcf"
    input:
        bam = "chunks/{base}.calling.bam",
        bai = "chunks/{base}.calling.bam.bai",
    log: "chunks/{base}.variants.called.vcf.log"
    threads: 2 if config["variant_caller"] in {"gatk", "bcftools"} else 1
    params:
        heap_space = config["heap_space"],
        reference = config["genome_reference"],
    run:
        heap_space = params.heap_space * threads
        commands = {
            "freebayes":
                 "freebayes"
                 " -f {params.reference}"
                 " {input.bam}"
                 " 2> {log}"
                 " | "
                 # Filter out variants with allele count above 0 as these are detected as polyploid by hapcut2
                 "vcffilter"
                 " -f"
                 " 'AC > 0'"
                 " 1> {output.vcf} 2>> {log}",
            "bcftools":
                "bcftools mpileup"
                " -f {params.reference}"
                " {input.bam}"
                " --threads {threads}"
                " 2> {log}"
                " | "
                "bcftools call"
                " -m"
                " -v"
                " -O v"
                " --ploidy GRCh38"
                " --threads {threads}"
                " -o {output.vcf} 2>> {log}",
            "gatk":
                "gatk --java-options '-Xmx{heap_space}g -XX:ParallelGCThreads={threads}' HaplotypeCaller"
                " -R {params.reference}"
                " -I {input.bam}"
                " -O {output.vcf}"
                " --create-output-variant-index false 2> {log}",
            # This option is currently not included in the pipeline documentation, but it will run on any system which
            # - runs Linux (it's not maintained for any other system)
            # - has deepvariant installed (e.g. through conda)
            # TODO: Add a conditional installation of deepvariant for Linux systems and add to pipeline docs.
            "deepvariant":
                "deepvariant"
                " --model_type=WGS"
                " --output_vcf={output.vcf}"
                " --reads={input.bam}"
                " --ref={params.reference}"
                }
        shell(commands[config["variant_caller"]])


rule extract_called:
    """Extract SNPs and INDELs from called varinats"""
    output:
        vcf = temporary("chunks/{base}.variants.called.{type,(SNP|INDEL)}.vcf")
    input:
        vcf = "chunks/{base}.variants.called.vcf"
    log: "chunks/{base}.variants.called.{type,(SNP|INDEL)}.vcf.log"
    shell:
        "gatk SelectVariants"
        " -V {input.vcf}"
        " -select-type {wildcards.type}"
        " --create-output-variant-index false"
        " -O {output.vcf} 2> {log}"


rule apply_filter_snps:
    """Hard filtering of SNPs"""
    output:
        vcf = temporary("chunks/{base}.variants.called.SNP.filtered.vcf")
    input:
        vcf = "chunks/{base}.variants.called.SNP.vcf"
    log: "chunks/{base}.variants.called.SNP.filtered.vcf.log"
    params:
        filters = parse_filters(config["hard_filters"]["snps"])
    shell:
        "gatk VariantFiltration"
        " -V {input.vcf}"
        " {params.filters}" 
        " --create-output-variant-index false"
        " -O {output.vcf} 2> {log}"


rule apply_filter_indels:
    """Hard filtering of INDELs"""
    output:
        vcf = temporary("chunks/{base}.variants.called.INDEL.filtered.vcf")
    input:
        vcf = "chunks/{base}.variants.called.INDEL.vcf"
    log: "chunks/{base}.variants.called.INDEL.filtered.vcf.log"
    params:
        filters = parse_filters(config["hard_filters"]["indels"])
    shell:
        "gatk VariantFiltration"
        " -V {input.vcf}"
        " {params.filters}" 
        " --create-output-variant-index false"
        " -O {output.vcf} 2> {log}"


rule filter_vcfs:
    """Remove VCF entries that don't pass filters."""
    output:
        vcf = temporary("chunks/{base}.variants.called.{type}.filtered.pass.vcf")
    input:
        vcf = "chunks/{base}.variants.called.{type}.filtered.vcf"
    shell:
        "bcftools view -f 'PASS' {input.vcf} > {output.vcf}"


rule merge_filtered_snps_indels:
    output:
        vcf = temporary("chunks/{base}.variants.called.filtered.vcf")
    input:
        vcf_snps = "chunks/{base}.variants.called.SNP.filtered.pass.vcf",
        vcf_indels = "chunks/{base}.variants.called.INDEL.filtered.pass.vcf"
    log: "chunks/{base}.variants.called.filtered.vcf.log"
    shell:
        "picard MergeVcfs"
        " --CREATE_INDEX false"
        " -O {output.vcf}"
        " -I {input.vcf_snps}"
        " -I {input.vcf_indels} 2> {log}"


def get_phase_input_vcf(wildcards):
    if config["reference_variants"]:
        return {
            "vcf": config["reference_variants"],
            "bed": f"chunks/{wildcards.chunk}.bed",
        }
    else:
        if config["filter_variants"]:
            return {"vcf": f"chunks/{wildcards.chunk}.variants.called.filtered.vcf"}
        else:
            return {"vcf": f"chunks/{wildcards.chunk}.variants.called.vcf"}


rule split_or_symlink_phase_input_vcf:
    output:
        vcf = temp("chunks/{chunk}.phaseinput.vcf")
    input:
        unpack(get_phase_input_vcf)
    run:
        if config["reference_variants"]:
            shell("bcftools view -o {output.vcf} -T {input.bed} {input.vcf}")
        else:
            symlink_relpath(input.vcf, output.vcf)


rule merge_bams:
    output:
        bam = "final.bam"
    input:
        bams = expand("chunks/{chunk[0].name}.calling.bam", chunk=chunks["all"]) +
               ["unmapped.bam"],
        bais = expand("chunks/{chunk[0].name}.calling.bam.bai", chunk=chunks["all"]) +
               ["unmapped.bam.bai"],
    shell:
        "samtools cat -o {output.bam} {input.bams}"


rule merge_bams_phased:
    output:
        cram = "final.phased.cram"
    input:
        bams = expand("chunks/{chunk[0].name}.calling.phased.bam", chunk=chunks["phased"]) +
               expand("chunks/{chunk[0].name}.calling.bam", chunk=chunks["not_phased"]) +
               ["unmapped.bam"],
        bais = expand("chunks/{chunk[0].name}.calling.phased.bam.bai", chunk=chunks["phased"]) +
               expand("chunks/{chunk[0].name}.calling.bam.bai", chunk=chunks["not_phased"]) +
               ["unmapped.bam.bai"]
    params:
        reference = config["genome_reference"],
    shell:
        "samtools cat {input.bams} | samtools view -T {params.reference} -C - > {output.cram}"


def vcfs_to_merge(wildcards):
    input = [f"chunks/{chunk[0].name}.calling.phased.vcf" for chunk in chunks["phased"]]
    if config["reference_variants"] is None:
        filt = "filtered." if config["filter_variants"] else ""
        input.extend([f"chunks/{chunk[0].name}.variants.called.{filt}vcf" for chunk in chunks["primary_not_phased"]])
    return input


rule concat_phased_vcfs:
    output:
        vcf = temp("final.phased.vcf")
    input:
        vcfs_to_merge
    log: "final.phased.vcf.log"
    shell:
        "bcftools concat -o {output.vcf} {input} 2> {log}"


rule concat_called_vcfs:
    output:
        vcf = temp("called.{filtered,(filtered.|)}vcf")
    input:
        vcf = expand("chunks/{chunk[0].name}.variants.called.{{filtered}}vcf", chunk=chunks["primary"])
    log: "called.{filtered,(filtered.|)}vcf.log"
    shell:
        "bcftools concat -o {output.vcf} {input.vcf} 2> {log}"


rule generate_primary_bed:
    """Generate BED of primary contigs"""
    output:
        bed = temp("primary.bed")
    input:
        beds = expand("chunks/{chunk[0].name}.bed", chunk=chunks["primary"])
    shell:
        "cat {input.beds} > {output.bed}"


rule mosdepth:
    """Calculate depth stats"""
    output:
        "final.mosdepth.global.dist.txt",
        "final.mosdepth.summary.txt",
    input:
        bam = "final.bam",
        bai = "final.bam.bai",
        bed = "primary.bed"
    params:
        prefix = "final"
    threads: 4
    shell:
        "mosdepth"
        " --threads {threads}"
        " --no-per-base"
        " --by {input.bed}"
        " {params.prefix}"
        " {input.bam}"


rule aggregate_phaseblock_data:
    output:
        tsv = "final.phaseblock_data.tsv"
    input:
        phase = expand("chunks/{chunk[0].name}.calling.phase", chunk=chunks["phased"])
    run:
        with open(output.tsv, "w") as output_tsv:
            print("Variants spanned", "Variants phased", "Length", "Fragments", sep="\t", file=output_tsv)
            for input_file in input.phase:
                with open(input_file) as file:
                    for p in parse_phaseblocks(file):
                        print(p.snv_span, p.phased_snvs, p.length, p.fragments, sep="\t", file=output_tsv)


rule aggregate_molecule_lengths:
    output:
        tsv = "final.molecule_lengths.tsv"
    input:
        tsv = "final.molecule_stats.filtered.tsv"
    run:
        molecules = pd.read_csv(input.tsv, sep="\t")
        bins = range(0, max(molecules["Length"])+1000, 1000)
        molecules["Bin"] = pd.cut(molecules["Length"], bins=bins, labels=bins[:-1])
        binned_lengths = molecules.groupby("Bin", as_index=False)["Length"].sum()
        binned_lengths.columns = ["Bin", "LengthSum"]
        binned_lengths = binned_lengths[binned_lengths["LengthSum"] > 0]  # Remove zero entries
        binned_lengths.to_csv(output.tsv, sep="\t", index=False)


rule generate_phase_chr_lengths:
    """Generate list of lengths for phased contigs"""
    output:
        txt = temp("phased_contig_lengths.txt")
    input:
        beds = expand("chunks/{chunk[0].name}.bed", chunk=chunks["phased"])
    shell:
        "cat {input.beds} | cut -f1,3 > {output.txt}"


rule whatshap_stats:
    """Calculate phaseing stats related to variants and phaseblocks."""
    output:
        tsv = "final.whatshap_stats.tsv"
    input:
        phased_vcf = "final.phased.vcf.gz",
        phased_vcf_index = "final.phased.vcf.gz.tbi",
        phased_contigs_lengths = "phased_contig_lengths.txt"
    log: "final.whatshap_stats.tsv.log"
    params:
        chromosomes = ' '.join([f"--chromosome {contig.name}" for chunk in chunks["phased"] for contig in chunk ])
    shell:
         "whatshap stats"
         " --chr-lengths {input.phased_contigs_lengths}"
         " {params.chromosomes}"
         " --tsv {output.tsv}"
         " {input.phased_vcf} &> {log}"


rule collect_duplicate_metrics:
    output:
        metrics = "final.duplicate_metrics.txt"
    input:
        bam = "final.bam"
    log: "final.duplicate_metrics.txt.log"
    params:
        java_args = f"-Xmx{config['heap_space']}g"
    shell:
        "picard {params.java_args} CollectDuplicateMetrics"
        " I={input.bam}"
        " M={output.metrics}"
        " ASSUME_SORTED=true"
        " VALIDATION_STRINGENCY=LENIENT 2> {log}"


rule collect_picard_metrics:
    """Collect insert size and alignment metrics from BAM file."""
    output:
        "final.alignment_summary_metrics.txt",
        "final.read_length_histogram.pdf",
        "final.insert_size_metrics.txt",
        "final.insert_size_histogram.pdf",
        "final.gc_bias.detail_metrics.txt",
        "final.gc_bias.summary_metrics.txt",
        "final.gc_bias.pdf"
    input:
        bam = "final.bam"
    log: "final.collect_picard_metrics.log"
    params:
        output_prefix = "final",
        reference = config["genome_reference"],
    shell:
        "picard CollectMultipleMetrics"
        " I={input.bam}"
        " O={params.output_prefix}"
        " PROGRAM=null"
        " PROGRAM=CollectAlignmentSummaryMetrics"
        " PROGRAM=CollectInsertSizeMetrics"
        " PROGRAM=CollectGcBiasMetrics"
        " VALIDATION_STRINGENCY=LENIENT"
        " FILE_EXTENSION=.txt"
        " EXTRA_ARGUMENT=CollectAlignmentSummaryMetrics::MAX_INSERT_SIZE=2000"
        " REFERENCE_SEQUENCE={params.reference}"
        " 2> {log}"


rule samtools_stats:
    output:
        "final.samtools_stats.txt"
    input:
        "final.bam"
    log: "final.samtools_stats.txt.log"
    threads: 4
    shell:
        "samtools stats -@ {threads} {input} > {output} 2> {log}"


rule concat_lsv_calls:
    output:
        tsv = "final.naibr_sv_calls.tsv"
    input:
        tsv = expand("chunks/{chunk[0].name}.naibr_sv_calls.tsv", chunk=chunks["primary"])
    run:
        dfs = list()
        for file in input.tsv:
            try:
                dfs.append(pd.read_csv(file, sep="\t"))
            except pd.errors.EmptyDataError:
                continue

        concat = pd.concat(dfs, ignore_index=True)
        concat.to_csv(output.tsv, sep="\t", index=False)
